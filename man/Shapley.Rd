% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Shapley.R
\name{Shapley}
\alias{Shapley}
\title{Game theoretic prediction explanations}
\format{\code{\link{R6Class}} object.}
\description{
\code{Shapley} computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory approach. 
The features values of an instance cooperate to achieve the prediction. 
The Shapley value fairly distributes the difference of the instance's prediction and the datasets average prediction among the features. 
A features contribution can be negative.
}
\section{Usage}{

\preformatted{
shapley = Shapley$new(model, data, x.interest = NULL, 
  sample.size = 100, run = TRUE)

plot(shapley)
shapley$results
print(shapley)
}
}

\section{Arguments}{
 
For Shapley$new():
\describe{
\item{model}{object of type \code{Model}. See \link{Model}.}
\item{data}{data.frame with the data to which compare the x.interest.}
\item{x.interest}{data.frame with a single row for the instance to be explained.}
\item{sample.size}{The number of instances to be sampled from the data.} 
\item{maxdepth}{The maximum depth of the tree. Default is 2.}
\item{run}{logical. Should the Interpretation method be run?}
}
}

\section{Details}{

For more details on the algorithm see https://christophm.github.io/interpretable-ml-book/shapley.html
}

\section{Fields}{

\describe{
\item{sample.size}{The number of times coalitions/marginals are sampled from data X. The higher the more accurate the explanations become.}
\item{x.interest}{data.frame with the instance of interest}
\item{y.hat.interest}{predicted value for instance of interest}
\item{y.hat.averate}{average predicted value for data \code{X}} 
\item{results}{data.frame with sampled feature X together with the leaf node information (columns ..node and ..path) 
and the predicted \eqn{\hat{y}} for tree and machine learning model (columns starting with ..y.hat).}
\item{x.interest}{data.frame with a single row for the instance to be explained.}
}
}

\section{Methods}{

\describe{
\item{explain(x.interest)}{method to set a new data point which to explain.}
\item{plot()}{method to plot the Shapley value. See \link{plot.Shapley}}
\item{\code{run()}}{[internal] method to run the interpretability method. Use \code{obj$run(force = TRUE)} to force a rerun.}
General R6 methods
\item{\code{clone()}}{[internal] method to clone the R6 object.}
\item{\code{initialize()}}{[internal] method to initialize the R6 object.}
}
}

\examples{
# First we fit a machine learning model on the Boston housing data
if (require("randomForest")) {
data("Boston", package  = "MASS")
rf =  randomForest(medv ~ ., data = Boston, ntree = 50)
mod = Model$new(rf)
X = Boston[-which(names(Boston) == "medv")]

# Then we explain the first instance of the dataset with the Shapley method:
x.interest = X[1,]
shapley = Shapley$new(mod, X, x.interest = x.interest)
shapley

# Look at the results in a table
shapley$results
# Or as a plot
plot(shapley)

# Shapley() also works with multiclass classification
library("randomForest")
rf = randomForest(Species ~ ., data= iris, ntree=50)
mod = Model$new(rf, predict.args = list(type='prob'))
X = iris[-which(names(iris) == "Species")]

# Then we explain the first instance of the dataset with the Shapley() method:
shapley = Shapley$new(mod, X, x.interest = X[1,])
shapley$results
plot(shapley) 

# You can also focus on one class
mod = Model$new(rf, predict.args = list(type='prob'), class = 2)
shapley = Shapley$new(mod, X, x.interest = X[1,])
shapley$results
plot(shapley) 
}
}
\references{
Strumbelj, E., Kononenko, I. (2014). Explaining prediction models and individual predictions with feature contributions. Knowledge and Information Systems, 41(3), 647-665. https://doi.org/10.1007/s10115-013-0679-x
}
\seealso{
\link{Lime}

A different way to explain predictions: \link{Lime}
}
