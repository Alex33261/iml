% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Shapley.R
\name{Shapley}
\alias{Shapley}
\title{Explain predictions}
\format{\code{\link{R6Class}} object.}
\description{
Explain predictions
}
\section{Description}{
 
Shapley() computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory approach. 
The features values of an instance cooperate to achieve the prediction. 
Shapley() fairly distributes the difference of the instance's prediction and the datasets average prediction among the features. 
A features contribution can be negative.
}

\section{Usage}{

\preformatted{
shapley = Shapley$new(predictor, data, x.interest, 
  sample.size = 100, class = NULL, run = TRUE)

plot(shapley)
shapley$data()
print(shapley)
}
}

\section{Arguments}{
 
#' For Shapley$new():
\describe{
\item{predictor}{
The machine learning model. Different types are allowed. 
Recommended are mlr WrappedModel and caret train objects. The \code{object} can also be 
a function that predicts the outcome given features or anything with an S3 predict function,
like an object from class \code{lm}.}
\item{data}{data.frame with the data for the prediction model}
\item{x.interest}{data.frame with a single row for the instance to be explained.}
\item{class}{In case of classification, class specifies the class for which to predict the probability. 
By default the multiclass classification is done.}
\item{sample.size}{The number of instances to be sampled from X.} 
\item{maxdepth}{The maximum depth of the tree. Default is 2.}
\item{class}{The class column that should be looked at from the prediction model. 
Ignored if output one-dimensional}
\item{run}{logical. Should the Interpretation method be run?}
}
}

\examples{
# First we fit a machine learning model on the Boston housing data
library("randomForest")
data("Boston", package  = "MASS")
mod = randomForest(medv ~ ., data = Boston, ntree = 50)
X = Boston[-which(names(Boston) == "medv")]

# Then we explain the first instance of the dataset with the Shapley method:
x.interest = X[1,]
shapley = Shapley$new(mod, X, x.interest = x.interest)
shapley

# Look at the results in a table
shapley$data()
# Or as a plot
plot(shapley)

# Shapley() also works with multiclass classification
library("randomForest")
mod = randomForest(Species ~ ., data= iris, ntree=50)
X = iris[-which(names(iris) == "Species")]

# Then we explain the first instance of the dataset with the makeShapley() method:
shapley = Shapley$new(mod, X, x.interest = X[1,], predict.args = list(type='prob'))
shapley$data()
plot(shapley) 

# You can also focus on one class
shapley = Shapley$new(mod, X, x.interest = X[1,], class = 2, predict.args = list(type='prob'))
shapley$data()
plot(shapley) 

}
\references{
Strumbelj, E., Kononenko, I. (2014). Explaining prediction models and individual predictions with feature contributions. Knowledge and Information Systems, 41(3), 647-665. https://doi.org/10.1007/s10115-013-0679-x
}
\seealso{
\link{Lime}

A different way to explain predictions: \link{Lime}
}
