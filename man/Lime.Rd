% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Lime.R
\name{Lime}
\alias{Lime}
\title{Lime}
\format{\code{\link{R6Class}} object.}
\description{
\code{Lime} fits a locally weighted linear regression model (logistic for classification) to explain a single machine learning prediction.
}
\section{Usage}{

\preformatted{
lime = Lime$new(model, data, x.interest, run = TRUE)

plot(lime)
predict(lime, newdata)
lime$data()
print(lime)
}
}

\section{Arguments}{


For Lime$new():
\describe{
\item{model}{Object of type \code{Model}. See \link{Model}.}
\item{data}{data.frame with the data for the prediction model.}
\item{x.interest}{data.frame with a single row for the instance to be explained.}
\item{k}{the (maximum) number of features to be used for the surrogate model.}
\item{run}{logical. Should the Interpretation method be run?}
}
}

\section{Details}{
 
Data points are sampled and weighted by their proximity to the instance to be explained. 
A weighted glm is fitted with the machine learning model prediction as target. 
L1-regularisation is used to make the results sparse. 
The resulting model can be seen as a surrogate for the machine learning model, which is only valid for that one point.
Categorical features are binarized, depending on the category of the instance to be explained: 1 if the category is the same, 0 otherwise.
To learn more about local models, read the Interpretable Machine Learning book: https://christophm.github.io/interpretable-ml-book/lime.html

Differences to the original Lime implementation: 
\itemize{
\item Distance measure: Uses gower proximity (= 1 - gower distance) instead of a kernel based on the Euclidean distance. Has the advantage to have a meaningful neighbourhood and no kernel width to tune.
\item Sampling: Uses the original data instead of sampling from normal distributions. 
Has the advantage to follow the original data distribution. 
\item Visualisation: Plots effects instead of betas. Is the same for binary features, but makes a difference for numerical features. 
For numerical features, plotting the betas makes no sense, 
because a negative beta might still increase the prediction when the feature value is also negative.
}
}

\section{Fields}{

\describe{
\item{model}{the glmnet object.}
\item{best.fit.index}{the index of the best glmnet fit}
\item{k}{The number of features as set by the user.}
}
}

\section{Methods}{

\describe{
\item{x.interest}{method to get/set the instance. See examples for usage.}
\item{data()}{method to extract the results of the local feature effects 
Returns a data.frame with the feature names (\code{feature}) and contributions to the prediction}
\item{plot()}{method to plot the Lime feature effects. See \link{plot.Lime}}
\item{predict()}{method to predict new data with the local model See also \link{predict.Lime}}
\item{\code{run()}}{[internal] method to run the interpretability method. Use \code{obj$run(force = TRUE)} to force a rerun.}
General R6 methods
\item{\code{clone()}}{[internal] method to clone the R6 object.}
\item{\code{initialize()}}{[internal] method to initialize the R6 object.}
}
}

\examples{
# First we fit a machine learning model on the Boston housing data
library("randomForest")
data("Boston", package  = "MASS")
X = Boston[-which(names(Boston) == "medv")]
rf = randomForest(medv ~ ., data = Boston, ntree = 50)
mod = Model$new(rf)

# Then we explain the first instance of the dataset with the Lime method:
x.interest = X[1,]
lemon = Lime$new(mod, X, x.interest = x.interest, k = 2)
lemon

# Look at the results in a table
lemon$data()
# Or as a plot
plot(lemon)

# Reuse the object with a new instance to explain
lemon$x.interest = X[2,]
plot(lemon)
  
# Lime also works with multiclass classification
library("randomForest")
rf = randomForest(Species ~ ., data= iris, ntree=50)
mod = Model$new(rf,predict.args = list(type='prob'), class = 2)
X = iris[-which(names(iris) == 'Species')]

# Then we explain the first instance of the dataset with the Lime method:
lemon = Lime$new(mod, X, x.interest = X[1,], k = 2)
lemon$data()
plot(lemon) 

}
\references{
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier. Retrieved from http://arxiv.org/abs/1602.04938
}
\seealso{
\code{\link{plot.Lime}} and \code{\link{predict.Lime}}

\code{\link{Shapley}} can also be used to explain single predictions

\code{\link[lime]{lime}}, the original implementation
}
