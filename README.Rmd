---
output: github_document
---
[![Build Status](https://travis-ci.org/christophM/iml.svg?branch=master)](https://travis-ci.org/christophM/iml)

# iml: interpretable machine learning
 `iml ` is an R package that interprets the behaviour and explains predictions of machine learning models.
 It implements model-agnostic interpretability methods - meaning they can be used with any machine learning model.
 
 Currently implemented: 
 
 - Feature importance
 - Partial dependence plots
 - Individual conditional expectation plots (ICE)
 - Tree surrogate
 - LIME: Local Interpretable Model-agnostic Explanations
 - Shapley value for explaining single predictions
 
Read more about the methods in the [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book/agnostic.html)

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
set.seed(42)
```

# Installation 
The package can be installed directly from github with devtools:
```{r, results = 'hide', eval = FALSE}
# install.packages("devtools")
devtools::install_github('christophM/iml')
```


# Examples

First we train a randomForest to predict the Boston median housing value
```{r}
library('iml')

library('randomForest')
data("Boston", package  = "MASS")
mod = randomForest(medv ~ ., data = Boston, ntree = 50)
```

#### What were the most important features? (Permutation feature importance / Model reliance)
```{r}
imp = feature.imp(mod, Boston, y = Boston$medv, loss = 'mae')
plot(imp)
imp$data()
```

### Let's build a single tree from the randomForest predictions! (Tree surrogate)
```{r}
tree = tree.surrogate(mod, Boston[which(names(Boston) != 'medv')], maxdepth = 2)
plot(tree)
```

### How does lstat influence the prediction on average? (Partial dependence plot)
```{r}
pdp.obj = pdp(mod, Boston, feature = 13)
plot(pdp.obj)
```


### How does lstat influence the individual predictions? (ICE)
```{r}
ice.curves = ice(mod, Boston[1:100,], feature = 13)
plot(ice.curves) 
```



### Explain a single prediction with a local linear model. (LIME) 
```{r}
x = Boston[1,]
lime.explain = lime(mod, Boston, x.interest = x)
lime.explain$data()
plot(lime.explain)
```



### Explain a single prediction with game theory. (Shapley)
```{r}
x = Boston[1,]
shapley.explain = shapley(mod, Boston, x.interest = x)
shapley.explain$data()
plot(shapley.explain)

```

# Python Implementation
Referring to https://github.com/datascienceinc/Skater


