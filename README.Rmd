---
output: github_document
---
[![Build Status](https://travis-ci.org/christophM/iml.svg?branch=master)](https://travis-ci.org/christophM/iml)

# iml: interpretable machine learning
 `iml ` is an R package that interprets the behaviour and explains predictions of machine learning models.
 It implements model-agnostic interpretability methods - meaning they can be used with any machine learning model.
 
 Currently implemented: 
 
 - Feature importance
 - Partial dependence plots
 - Individual conditional expectation plots (ICE)
 - Tree surrogate
 - LocalModel: Local Interpretable Model-agnostic Explanations
 - Shapley value for explaining single predictions
 
Read more about the methods in the [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book/agnostic.html)

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
set.seed(42)
```

# Installation 
The package can be installed directly from github with devtools:
```{r, results = "hide", eval = FALSE}
# install.packages("devtools")
devtools::install_github("christophM/iml")
```


# Examples

First we train a randomForest to predict the Boston median housing value
```{r}
library("iml")

library("randomForest")
data("Boston", package  = "MASS")
rf = randomForest(medv ~ ., data = Boston, ntree = 50)
X =  Boston[which(names(Boston) != "medv")]
model = Predictor$new(rf, data = X, y = Boston$medv)
```

#### What were the most important features? (Permutation feature importance / Model reliance)
```{r}
imp = FeatureImp$new(model, loss = "mae")
plot(imp)
imp$results
```

### Let"s build a single tree from the randomForest predictions! (Tree surrogate)
```{r}
tree = TreeSurrogate$new(model, maxdepth = 2)
plot(tree)
```

### How does lstat influence the prediction on average? (Partial dependence plot)
```{r}
pdp.obj = PartialDependence$new(model, feature = "lstat")
plot(pdp.obj)
```


### How does lstat influence the individual predictions? (ICE)
```{r}
ice.curves = Ice$new(model, feature = "lstat")
plot(ice.curves) 
```



### Explain a single prediction with a local linear model. (LIME) 
```{r}
lime.explain = LocalModel$new(model, x.interest = X[1,])
lime.explain$results
plot(lime.explain)
```



### Explain a single prediction with game theory. (Shapley)
```{r}
shapley.explain = Shapley$new(model, x.interest = X[1, ])
shapley.explain$results
plot(shapley.explain)

```
# Versions
## 0.2
- Big API changes: 
  - direct use of R6 classes (e.g. Ice$new() instead of ice()) to reduce redundancy
  - Lime was renamed to LocalModel to reflect the differences
  - results from the object can be retrieved using $results instead of $data()
- Improved the plots
- Submitted to CRAN
- 
## 0.1
- Initial release

# Python Implementation
Referring to https://github.com/datascienceinc/Skater


