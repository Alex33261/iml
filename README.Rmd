---
output: github_document
---
[![Build Status](https://travis-ci.org/christophM/iml.svg?branch=master)](https://travis-ci.org/christophM/iml)
[![CRAN Status Badge](http://www.r-pkg.org/badges/version/iml)](https://CRAN.R-project.org/package=iml)
[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/grand-total/iml)](https://cran.rstudio.com/web/packages/iml/index.html)


# iml: interpretable machine learning
 `iml ` is an R package that interprets the behaviour and explains predictions of machine learning models.
 It implements model-agnostic interpretability methods - meaning they can be used with any machine learning model.
 
 Currently implemented: 
 
 - Feature importance
 - Partial dependence plots
 - Individual conditional expectation plots (ICE)
 - Tree surrogate
 - LocalModel: Local Interpretable Model-agnostic Explanations
 - Shapley value for explaining single predictions
 
Read more about the methods in the [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book/agnostic.html)



```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
set.seed(42)
```

# Installation 
The package can be installed directly from github with devtools:
```{r, results = "hide", eval = FALSE}
# install.packages("devtools")
devtools::install_github("christophM/iml")
```

# News
Changes of the packages can be accessed in the [NEWS file](https://github.com/christophM/iml/blob/master/NEWS.md) shipped with the package.

# Examples

First we train a randomForest to predict the Boston median housing value
```{r}
library("iml")

library("randomForest")
data("Boston", package  = "MASS")
rf = randomForest(medv ~ ., data = Boston, ntree = 50)
X =  Boston[which(names(Boston) != "medv")]
model = Predictor$new(rf, data = X, y = Boston$medv)
```

#### What were the most important features? (Permutation feature importance / Model reliance)
```{r}
imp = FeatureImp$new(model, loss = "mae")
plot(imp)
imp$results
```

### Let"s build a single tree from the randomForest predictions! (Tree surrogate)
```{r}
tree = TreeSurrogate$new(model, maxdepth = 2)
plot(tree)
```

### How does lstat influence the prediction on average? (Partial dependence plot)
```{r}
pdp.obj = PartialDependence$new(model, feature = "lstat")
plot(pdp.obj)
```


### How does lstat influence the individual predictions? (ICE)
```{r}
ice.curves = Ice$new(model, feature = "lstat")
plot(ice.curves) 
```



### Explain a single prediction with a local linear model. (LIME) 
```{r}
lime.explain = LocalModel$new(model, x.interest = X[1,])
lime.explain$results
plot(lime.explain)
```



### Explain a single prediction with game theory. (Shapley)
```{r}
shapley.explain = Shapley$new(model, x.interest = X[1, ])
shapley.explain$results
plot(shapley.explain)

```

# Python Implementation
Referring to https://github.com/datascienceinc/Skater


