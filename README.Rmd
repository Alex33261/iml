---
output: github_document
---
[![Build Status](https://travis-ci.org/christophM/iml.svg?branch=master)](https://travis-ci.org/christophM/iml)

# iml: interpretable machine learning
 `iml ` is an R package that interprets the behaviour and explains predictions of machine learning models.
 It implements model-agnostic interpretability methods - meaning they can be used with any machine learning model.
 
 Currently implemented: 
 
 - Feature importance
 - Partial dependence plots
 - Individual conditional expectation plots (ICE)
 - Tree surrogate
 - LIME: Local Interpretable Model-agnostic Explanations
 - Shapley value for explaining single predictions
 
Read more about the methods in the [Interpretable Machine Learning book](https://christophm.github.io/interpretable-ml-book/agnostic.html)

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
set.seed(42)
```

# Installation 
The package can be installed directly from github with devtools:
```{r, results = "hide", eval = FALSE}
# install.packages("devtools")
devtools::install_github("christophM/iml")
```


# Examples

First we train a randomForest to predict the Boston median housing value
```{r}
library("iml")

library("randomForest")
data("Boston", package  = "MASS")
rf = randomForest(medv ~ ., data = Boston, ntree = 50)
model = Predictor$new(rf)
```

#### What were the most important features? (Permutation feature importance / Model reliance)
```{r}
imp = FeatureImp$new(model, Boston, y = Boston$medv, loss = "mae")
plot(imp)
imp$results
```

### Let"s build a single tree from the randomForest predictions! (Tree surrogate)
```{r}
tree = TreeSurrogate$new(model, Boston[which(names(Boston) != "medv")], maxdepth = 2)
plot(tree)
```

### How does lstat influence the prediction on average? (Partial dependence plot)
```{r}
pdp.obj = PartialDependence$new(model, Boston, feature = "lstat")
plot(pdp.obj)
```


### How does lstat influence the individual predictions? (ICE)
```{r}
ice.curves = Ice$new(model, Boston[1:100,], feature = "lstat")
plot(ice.curves) 
```



### Explain a single prediction with a local linear model. (LIME) 
```{r}
lime.explain = Lime$new(model, Boston, x.interest = Boston[1,])
lime.explain$results
plot(lime.explain)
```



### Explain a single prediction with game theory. (Shapley)
```{r}
shapley.explain = Shapley$new(model, Boston, x.interest = Boston[1, ])
shapley.explain$results
plot(shapley.explain)

```

# Python Implementation
Referring to https://github.com/datascienceinc/Skater


